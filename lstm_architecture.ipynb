{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_architecture.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZvYysvL8mLXQ96jv+ZLXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpordoy/Stacked_BiDirectioal_LSTM_NeuralNetwork/blob/main/lstm_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0mnq-_zEoa-"
      },
      "source": [
        "Neural network's architecture\n",
        "Mainly, the number of stacked and residual layers can be parametrized easily as well as whether or not bidirectional LSTM cells are to be used. \n",
        "Input data needs to be windowed to an array with one more dimension: the training and testing is never done on full signal lengths and use shuffling with resets of the hidden cells' states.\n",
        "We are using a deep neural network with stacked LSTM cells as well as residual (highway) LSTM cells for every stacked layer, a little bit like in ResNet, but for RNNs.\n",
        "Our LSTM cells are also bidirectional in term of how they pass trough the time axis, but differ from classic bidirectional LSTMs by the fact we concatenate their output features rather than adding them in an element-wise fashion. \n",
        "A simple hidden ReLU layer then lowers the dimension of those concatenated features for sending them to the next stacked layer. Bidirectionality can be disabled easily."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOkAU5lOl3N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2MQ945yINP0",
        "outputId": "bd444603-c622-42e6-d070-851051e0c721"
      },
      "source": [
        "#imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "print(\"Imports Imported\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports Imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROCxU5Z8ONFJ"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOQvsvUiJ0d8"
      },
      "source": [
        "# One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction.\n",
        "def onehot(y):\n",
        "  \"\"\"convert label from dense to one hot\n",
        "  argument:\n",
        "    label: ndarray dense label, shape: [sample_num1]\n",
        "  return:\n",
        "    one_hot_label: ndarray one hot, shape:  [sample_num, n_class]\n",
        "  \"\"\"\n",
        "  #numpy.reshape gves a new shape to an array without changing its data and return the length of y\n",
        "  y=y.reshape(len(y))\n",
        "  # np.max is a function that works on a single input array and finds the value of maximum element in that entire array\n",
        "  n_values=np.max(y)+1\n",
        "  #np.eye Returns a 2-D array with ones on the diagonal and zeros elsewhere.\n",
        "  return np.eye(n_values)[np.array(y, dtype=np.int32)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xll_xJyYJ0oW"
      },
      "source": [
        "# Implementing batch normalisation: this is used out of the residual layers\n",
        "# to normalise those output neurons by mean and standard dzeviation.\n",
        "def batch_norm(input_tensor, config, i):\n",
        "  if config.n_layers_in_highway == 0:\n",
        "      # There is no residual layers, no need for batch_norm:\n",
        "      return input_tensor\n",
        "\n",
        "  with tf.variable_scope(\"\") as scope:\n",
        "    if i!=0:\n",
        "      scope.reuse_varibales()\n",
        "\n",
        "    # Mean and variance normalisation  simply crunched over all axes\n",
        "    # Variance normalisation removes almost all the redundancy of any order from the input, permitting a continuous adaptation to the input range\n",
        "    # Mean Normalization is a way to implement Feature Scaling. What Mean normalization does is that it calculates and subtracts the mean for every feature. \n",
        "    # A common practice is also to divide this value by the range or the standard deviation.\n",
        "    axes = list(range(len(input_tensor.get_shape())))\n",
        "\n",
        "    mean, variance = tf.nn.moments(input_tensor, axes=axes, shift=None, keep_dims=False)\n",
        "    stdev = tf.sqrt(variance+0.001)\n",
        "\n",
        "    #Rescaling\n",
        "    bn = input_tensor - mean\n",
        "    bn /= stdev\n",
        "    # Learnable extra rescaling\n",
        "\n",
        "    # tf.get_variable(\"relu_fc_weights\", initializer=tf.random_normal(mean=0.0, stddev=0.0)\n",
        "    bn *= tf.get_variable(\"a_noreg\", initializer=tf.random_normal([1], mean=0.5, stddev=0.0))\n",
        "    bn += tf.get_variable(\"b_noreg\", initializer=tf.random_normal([1], mean=0.0, stddev=0.0))\n",
        "    # bn *= tf.Variable(0.5, name=(scope.name + \"/a_noreg\"))\n",
        "    # bn += tf.Variable(0.0, name=(scope.name + \"/b_noreg\"))\n",
        "\n",
        "  return bn\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qErsqMkRJ0sM"
      },
      "source": [
        "def relu_fc(input_2D_tensor_list, features_len, new_features_len, config):\n",
        "    \"\"\"make a relu fully-connected layer, mainly change the shape of tensor\n",
        "       both input and output is a list of tensor\n",
        "        argument:\n",
        "            input_2D_tensor_list: list shape is [batch_size,feature_num]\n",
        "            features_len: int the initial features length of input_2D_tensor\n",
        "            new_feature_len: int the final features length of output_2D_tensor\n",
        "            config: Config used for weights initializers\n",
        "        return:\n",
        "            output_2D_tensor_list lit shape is [batch_size,new_feature_len]\n",
        "    \"\"\"\n",
        "\n",
        "    W = tf.get_variable(\n",
        "        \"relu_fc_weights\",\n",
        "        initializer=tf.random_normal(\n",
        "            [features_len, new_features_len],\n",
        "            mean=0.0,\n",
        "            stddev=float(config.weights_stddev)\n",
        "        )\n",
        "    )\n",
        "    b = tf.get_variable(\n",
        "        \"relu_fc_biases_noreg\",\n",
        "        initializer=tf.random_normal(\n",
        "            [new_features_len],\n",
        "            mean=float(config.bias_mean),\n",
        "            stddev=float(config.weights_stddev)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # intra-timestep multiplication:\n",
        "    output_2D_tensor_list = [\n",
        "        tf.nn.relu(tf.matmul(input_2D_tensor, W) + b)\n",
        "            for input_2D_tensor in input_2D_tensor_list\n",
        "    ]\n",
        "\n",
        "    return output_2D_tensor_list\n",
        "\n",
        "\n",
        "def single_LSTM_cell(input_hidden_tensor, n_outputs):\n",
        "    \"\"\" define the basic LSTM layer\n",
        "        argument:\n",
        "            input_hidden_tensor: list a list of tensor,\n",
        "                                 shape: time_steps*[batch_size,n_inputs]\n",
        "            n_outputs: int num of LSTM layer output\n",
        "        return:\n",
        "            outputs: list a time_steps list of tensor,\n",
        "                     shape: time_steps*[batch_size,n_outputs]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(\"lstm_cell\"):\n",
        "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_outputs, state_is_tuple=True, forget_bias=0.999)\n",
        "        outputs, _ = tf.nn.rnn(lstm_cell, input_hidden_tensor, dtype=tf.float32)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def bi_LSTM_cell(input_hidden_tensor, n_inputs, n_outputs, config):\n",
        "    \"\"\"build bi-LSTM, concatenating the two directions in an inner manner.\n",
        "        argument:\n",
        "            input_hidden_tensor: list a time_steps series of tensor, shape: [sample_num, n_inputs]\n",
        "            n_inputs: int units of input tensor\n",
        "            n_outputs: int units of output tensor, each bi-LSTM will have half those internal units\n",
        "            config: Config used for the relu_fc\n",
        "        return:\n",
        "            layer_hidden_outputs: list a time_steps series of tensor, shape: [sample_num, n_outputs]\n",
        "    \"\"\"\n",
        "    n_outputs = int(n_outputs/2)\n",
        "\n",
        "    print(\"bidir:\")\n",
        "\n",
        "    with tf.variable_scope('pass_forward') as scope2:\n",
        "        hidden_forward = relu_fc(input_hidden_tensor, n_inputs, n_outputs, config)\n",
        "        forward = single_LSTM_cell(hidden_forward, n_outputs)\n",
        "\n",
        "    print (len(hidden_forward), str(hidden_forward[0].get_shape()))\n",
        "\n",
        "    # Backward pass is as simple as surrounding the cell with a double inversion:\n",
        "    with tf.variable_scope('pass_backward') as scope2:\n",
        "        hidden_backward = relu_fc(input_hidden_tensor, n_inputs, n_outputs, config)\n",
        "        backward = list(reversed(single_LSTM_cell(list(reversed(hidden_backward)), n_outputs)))\n",
        "\n",
        "    with tf.variable_scope('bidir_concat') as scope:\n",
        "        # Simply concatenating cells' outputs at each timesteps on the innermost\n",
        "        # dimension, like if the two cells acted as one cell\n",
        "        # with twice the n_hidden size:\n",
        "        layer_hidden_outputs = [\n",
        "            tf.concat(len(f.get_shape()) - 1, [f, b])\n",
        "                for f, b in zip(forward, backward)]\n",
        "\n",
        "    return layer_hidden_outputs\n",
        "\n",
        "\n",
        "def residual_bidirectional_LSTM_layers(input_hidden_tensor, n_input, n_output, layer_level, config, keep_prob_for_dropout):\n",
        "    \"\"\"This architecture is only enabled if \"config.n_layers_in_highway\" has a\n",
        "    value only greater than int(0). The arguments are same than for bi_LSTM_cell.\n",
        "    arguments:\n",
        "        input_hidden_tensor: list a time_steps series of tensor, shape: [sample_num, n_inputs]\n",
        "        n_inputs: int units of input tensor\n",
        "        n_outputs: int units of output tensor, each bi-LSTM will have half those internal units\n",
        "        config: Config used for determining if there are residual connections and if yes, their number and with some batch_norm.\n",
        "    return:\n",
        "        layer_hidden_outputs: list a time_steps series of tensor, shape: [sample_num, n_outputs]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('layer_{}'.format(layer_level)) as scope:\n",
        "\n",
        "        if config.use_bidirectionnal_cells:\n",
        "            get_lstm = lambda input_tensor: bi_LSTM_cell(input_tensor, n_input, n_output, config)\n",
        "        else:\n",
        "            get_lstm = lambda input_tensor: single_LSTM_cell(relu_fc(input_tensor, n_input, n_output, config), n_output)\n",
        "        def add_highway_redisual(layer, residual_minilayer):\n",
        "            return [a + b for a, b in zip(layer, residual_minilayer)]\n",
        "\n",
        "        hidden_LSTM_layer = get_lstm(input_hidden_tensor)\n",
        "        # Adding K new (residual bidir) connections to this first layer:\n",
        "        for i in range(config.n_layers_in_highway - 1):\n",
        "            with tf.variable_scope('LSTM_residual_{}'.format(i)) as scope2:\n",
        "                hidden_LSTM_layer = add_highway_redisual(\n",
        "                    hidden_LSTM_layer,\n",
        "                    get_lstm(input_hidden_tensor)\n",
        "                )\n",
        "\n",
        "        if config.also_add_dropout_between_stacked_cells:\n",
        "            hidden_LSTM_layer = [tf.nn.dropout(out, keep_prob_for_dropout) for out in hidden_LSTM_layer]\n",
        "\n",
        "        return [batch_norm(out, config, i) for i, out in enumerate(hidden_LSTM_layer)]\n",
        "\n",
        "\n",
        "def LSTM_network(feature_mat, config, keep_prob_for_dropout):\n",
        "    \"\"\"model a LSTM Network,\n",
        "      it stacks 2 LSTM layers, each layer has n_hidden=32 cells\n",
        "       and 1 output layer, it is a full connet layer\n",
        "      argument:\n",
        "        feature_mat: ndarray fature matrix, shape=[batch_size,time_steps,n_inputs]\n",
        "        config: class containing config of network\n",
        "      return:\n",
        "              : ndarray  output shape [batch_size, n_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope('LSTM_network') as scope:  # TensorFlow graph naming\n",
        "\n",
        "        feature_mat = tf.nn.dropout(feature_mat, keep_prob_for_dropout)\n",
        "\n",
        "        # Exchange dim 1 and dim 0\n",
        "        feature_mat = tf.transpose(feature_mat, [1, 0, 2])\n",
        "        print (feature_mat.get_shape())\n",
        "        # New feature_mat's shape: [time_steps, batch_size, n_inputs]\n",
        "\n",
        "        # Temporarily crush the feature_mat's dimensions\n",
        "        feature_mat = tf.reshape(feature_mat, [-1, config.n_inputs])\n",
        "        print (feature_mat.get_shape())\n",
        "        # New feature_mat's shape: [time_steps*batch_size, n_inputs]\n",
        "\n",
        "        # Split the series because the rnn cell needs time_steps features, each of shape:\n",
        "        hidden = tf.split(0, config.n_steps, feature_mat)\n",
        "        print (len(hidden), str(hidden[0].get_shape()))\n",
        "        # New shape: a list of lenght \"time_step\" containing tensors of shape [batch_size, n_hidden]\n",
        "\n",
        "        # Stacking LSTM cells, at least one is stacked:\n",
        "        print (\"\\nCreating hidden #1:\")\n",
        "        hidden = residual_bidirectional_LSTM_layers(hidden, config.n_inputs, config.n_hidden, 1, config, keep_prob_for_dropout)\n",
        "        print (len(hidden), str(hidden[0].get_shape()))\n",
        "\n",
        "        for stacked_hidden_index in range(config.n_stacked_layers - 1):\n",
        "            # If the config permits it, we stack more lstm cells:\n",
        "            print (\"\\nCreating hidden #{}:\".format(stacked_hidden_index+2))\n",
        "            hidden = residual_bidirectional_LSTM_layers(hidden, config.n_hidden, config.n_hidden, stacked_hidden_index+2, config, keep_prob_for_dropout)\n",
        "            print (len(hidden), str(hidden[0].get_shape()))\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        # Final fully-connected activation logits\n",
        "        # Get the last output tensor of the inner loop output series, of shape [batch_size, n_classes]\n",
        "        last_hidden = tf.nn.dropout(hidden[-1], keep_prob_for_dropout)\n",
        "        last_logits = relu_fc(\n",
        "            [last_hidden],\n",
        "            config.n_hidden, config.n_classes, config\n",
        "        )[0]\n",
        "        return last_logits\n",
        "\n",
        "\n",
        "def run_with_config(Config, X_train, y_train, X_test, y_test):\n",
        "    tf.reset_default_graph()  # To enable to run multiple things in a loop\n",
        "\n",
        "    #-----------------------------------\n",
        "    # Define parameters for model\n",
        "    #-----------------------------------\n",
        "    config = Config(X_train, X_test)\n",
        "    print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "    print(\"features shape, labels shape, each features mean, each features standard deviation\")\n",
        "    print(X_test.shape, y_test.shape,\n",
        "          np.mean(X_test), np.std(X_test))\n",
        "    print(\"the dataset is therefore properly normalised, as expected.\")\n",
        "\n",
        "    #------------------------------------------------------\n",
        "    # Let's get serious and build the neural network\n",
        "    #------------------------------------------------------\n",
        "    with tf.device(\"/cpu:0\"):  # Remove this line to use GPU. If you have a too small GPU, it crashes.\n",
        "        X = tf.placeholder(tf.float32, [\n",
        "                           None, config.n_steps, config.n_inputs], name=\"X\")\n",
        "        Y = tf.placeholder(tf.float32, [\n",
        "                           None, config.n_classes], name=\"Y\")\n",
        "\n",
        "        # is_train for dropout control:\n",
        "        is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
        "        keep_prob_for_dropout = tf.cond(is_train,\n",
        "            lambda: tf.constant(\n",
        "                config.keep_prob_for_dropout,\n",
        "                name=\"keep_prob_for_dropout\"\n",
        "            ),\n",
        "            lambda: tf.constant(\n",
        "                1.0,\n",
        "                name=\"keep_prob_for_dropout\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        pred_y = LSTM_network(X, config, keep_prob_for_dropout)\n",
        "\n",
        "        # Loss, optimizer, evaluation\n",
        "\n",
        "        # Softmax loss with L2 and L1 layer-wise regularisation\n",
        "        print (\"Unregularised variables:\")\n",
        "        for unreg in [tf_var.name for tf_var in tf.trainable_variables() if (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)]:\n",
        "            print (unreg)\n",
        "        l2 = config.lambda_loss_amount * sum(\n",
        "            tf.nn.l2_loss(tf_var)\n",
        "                for tf_var in tf.trainable_variables()\n",
        "                if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
        "        )\n",
        "        # first_weights = [w for w in tf.all_variables() if w.name == 'LSTM_network/layer_1/pass_forward/relu_fc_weights:0'][0]\n",
        "        # l1 = config.lambda_loss_amount * tf.reduce_mean(tf.abs(first_weights))\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits(pred_y, Y)) + l2  # + l1\n",
        "\n",
        "        # Gradient clipping Adam optimizer with gradient noise\n",
        "        optimize = tf.contrib.layers.optimize_loss(\n",
        "            loss,\n",
        "            global_step=tf.Variable(0),\n",
        "            learning_rate=config.learning_rate,\n",
        "            optimizer=tf.train.AdamOptimizer(learning_rate=config.learning_rate),\n",
        "            clip_gradients=config.clip_gradients,\n",
        "            gradient_noise_scale=config.gradient_noise_scale\n",
        "        )\n",
        "\n",
        "        correct_pred = tf.equal(tf.argmax(pred_y, 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
        "\n",
        "    #--------------------------------------------\n",
        "    # Hooray, now train the neural network\n",
        "    #--------------------------------------------\n",
        "    # Note that log_device_placement can be turned of for less console spam.\n",
        "\n",
        "    sessconfig = tf.ConfigProto(log_device_placement=False)\n",
        "    with tf.Session(config=sessconfig) as sess:\n",
        "        tf.initialize_all_variables().run()\n",
        "\n",
        "        best_accuracy = (0.0, \"iter: -1\")\n",
        "        best_f1_score = (0.0, \"iter: -1\")\n",
        "\n",
        "        # Start training for each batch and loop epochs\n",
        "\n",
        "        worst_batches = []\n",
        "\n",
        "        for i in range(config.training_epochs):\n",
        "\n",
        "            # Loop batches for an epoch:\n",
        "            shuffled_X, shuffled_y = shuffle(X_train, y_train, random_state=i*42)\n",
        "            for start, end in zip(range(0, config.train_count, config.batch_size),\n",
        "                                  range(config.batch_size, config.train_count + 1, config.batch_size)):\n",
        "\n",
        "                _, train_acc, train_loss, train_pred = sess.run(\n",
        "                    [optimize, accuracy, loss, pred_y],\n",
        "                    feed_dict={\n",
        "                        X: shuffled_X[start:end],\n",
        "                        Y: shuffled_y[start:end],\n",
        "                        is_train: True\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                worst_batches.append(\n",
        "                    (train_loss, shuffled_X[start:end], shuffled_y[start:end])\n",
        "                )\n",
        "                worst_batches = list(sorted(worst_batches))[-5:]  # Keep 5 poorest\n",
        "\n",
        "            # Train F1 score is not on boosting\n",
        "            train_f1_score = metrics.f1_score(\n",
        "                shuffled_y[start:end].argmax(1), train_pred.argmax(1), average=\"weighted\"\n",
        "            )\n",
        "\n",
        "            # Retrain on top worst batches of this epoch (boosting):\n",
        "            # a.k.a. \"focus on the hardest exercises while training\":\n",
        "            for _, x_, y_ in worst_batches:\n",
        "\n",
        "                _, train_acc, train_loss, train_pred = sess.run(\n",
        "                    [optimize, accuracy, loss, pred_y],\n",
        "                    feed_dict={\n",
        "                        X: x_,\n",
        "                        Y: y_,\n",
        "                        is_train: True\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Test completely at the end of every epoch:\n",
        "            # Calculate accuracy and F1 score\n",
        "            pred_out, accuracy_out, loss_out = sess.run(\n",
        "                [pred_y, accuracy, loss],\n",
        "                feed_dict={\n",
        "                    X: X_test,\n",
        "                    Y: y_test,\n",
        "                    is_train: False\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # \"y_test.argmax(1)\": could be optimised by being computed once...\n",
        "            f1_score_out = metrics.f1_score(\n",
        "                y_test.argmax(1), pred_out.argmax(1), average=\"weighted\"\n",
        "            )\n",
        "\n",
        "            print (\n",
        "                \"iter: {}, \".format(i) + \\\n",
        "                \"train loss: {}, \".format(train_loss) + \\\n",
        "                \"train accuracy: {}, \".format(train_acc) + \\\n",
        "                \"train F1-score: {}, \".format(train_f1_score) + \\\n",
        "                \"test loss: {}, \".format(loss_out) + \\\n",
        "                \"test accuracy: {}, \".format(accuracy_out) + \\\n",
        "                \"test F1-score: {}\".format(f1_score_out)\n",
        "            )\n",
        "\n",
        "            best_accuracy = max(best_accuracy, (accuracy_out, \"iter: {}\".format(i)))\n",
        "            best_f1_score = max(best_f1_score, (f1_score_out, \"iter: {}\".format(i)))\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"final test accuracy: {}\".format(accuracy_out))\n",
        "        print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n",
        "        print(\"final F1 score: {}\".format(f1_score_out))\n",
        "        print(\"best epoch's F1 score: {}\".format(best_f1_score))\n",
        "        print(\"\")\n",
        "\n",
        "    # returning both final and bests accuracies and f1 scores.\n",
        "    return accuracy_out, best_accuracy, f1_score_out, best_f1_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vETmOYTLJ0vr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlaVMBtAJ0zb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-K9TV-vJ02x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8RG3XHoJ1D1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT9F4CaqJ1IP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xag92UvzJ1Mj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqdT2llJ1P9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}